{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dots & Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is about making use of a Q-learning reinforcement algorithm to implement a game-playing agent which learns how to play a game of [3x3](https://www.wikihow.com/images/thumb/c/cb/Win-at-the-Dot-Game-Step-3.jpg/aid608874-v4-900px-Win-at-the-Dot-Game-Step-3.jpg) [Dots and Boxes](https://en.wikipedia.org/wiki/Dots_and_Boxes) optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3x3 Dots & Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dots & boxes is a 2-player game.\n",
    "\n",
    "The starting state is an empty grid of dots (16 dots in case of a 3x3 size board). Both players take turns making a move; a move consists of adding either a horizontal or vertical line between two unjoined adjacent dots. If making a move completes a 1x1 box, then the player who made that move wins that particular box (essentially, gets a point); the player also retains their turn. The game ends when there are no more available moves left to make. The player with the most points number of points is the winner of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining how to store and represent the game is a bit tricky, since both the dots and their intermediate edges are valid to the game state. However, representing both dots and edges is not feasible since doing so requires either multiple lists or nested ones, both of which are not unviable to use as input parameters to the neural network.\n",
    "\n",
    "One can, however, observe that the dots are constant for every state. Hence, a game state can be represented solely by its edges. All edges in the game are represented as a list (of length 24, since there are 24 edges in a 3x3 size game), with 0 denoting that an edge does not exist, and one denoting otherwise.\n",
    "\n",
    "The edge ordering being considered is:\n",
    "\n",
    "**&#183;**&nbsp;&nbsp;&nbsp; 0 &nbsp;&nbsp;&nbsp;**&#183;**&nbsp;&nbsp;&nbsp; 1 &nbsp;&nbsp;&nbsp;**&#183;**&nbsp;&nbsp;&nbsp; 2 &nbsp;&nbsp;&nbsp;**&#183;**  \n",
    "12 &nbsp;&nbsp;&nbsp;&nbsp; 13 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 14 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 15  \n",
    "**&#183;**&nbsp;&nbsp;&nbsp; 3 &nbsp;&nbsp;&nbsp;**&#183;**&nbsp;&nbsp;&nbsp; 4 &nbsp;&nbsp;&nbsp;**&#183;**&nbsp;&nbsp;&nbsp; 5 &nbsp;&nbsp;&nbsp;**&#183;**  \n",
    "16 &nbsp;&nbsp;&nbsp;&nbsp; 17 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 18 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 19  \n",
    "**&#183;**&nbsp;&nbsp;&nbsp; 6 &nbsp;&nbsp;&nbsp;**&#183;**&nbsp;&nbsp;&nbsp; 7 &nbsp;&nbsp;&nbsp;**&#183;**&nbsp;&nbsp;&nbsp; 8 &nbsp;&nbsp;&nbsp;**&#183;**  \n",
    "20 &nbsp;&nbsp;&nbsp;&nbsp; 21 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 22 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 23  \n",
    "**&#183;**&nbsp;&nbsp;&nbsp; 9 &nbsp;&nbsp;&nbsp;**&#183;**&nbsp;&nbsp; 10 &nbsp;&nbsp;**&#183;**&nbsp;&nbsp; 11 &nbsp;&nbsp;**&#183;**  \n",
    "\n",
    "The index of a value in this list represents the corresponding edge, e.g. if edge 0 exists in a state, then index 0 in the list has value 1, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A module containing simulation functions to train an AI agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. * `train_simulation(environment, train_agent, target_agent, n_games, update_step, test_agents=None, test_games=None)`: \n",
    "Runs a simulation to train an agent. If test agents are provided, tests the agent at an interval of 1000 iterations and logs the results.\n",
    "\n",
    "        * `environment`: game environment\n",
    "        * `train_agent`: learning agent\n",
    "        * `target_agent`: opponent agent\n",
    "        * `n_games`: number of training games to run\n",
    "        * `update_step`: number of games played until opponent model is updated\n",
    "        * `test_agents`: array of test agents\n",
    "        * `test_games`: number of games to play against test agents\n",
    "        * `return`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. * `output_comparison(training_games=10000, test_games=1000)`:\n",
    "Runs a comparison against 2 agents that are identical aside from the output activation function.\n",
    "    * `training_games`: Number of games to play\n",
    "    * `test_games`: Number of test games to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Directory is: .\\models\\size3\\\n",
      "Log file is: .\\models\\size3\\logs.txt\n",
      "WARNING:tensorflow:From C:\\Users\\gunja\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\gunja\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from .\\models\\size3\\-10000\n",
      "INFO:tensorflow:Restoring parameters from .\\models\\size3\\-10000\n",
      "Load Succeeded\n",
      "Starting at game 10001\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ai_agents import DQNLearner\n",
    "from naive_players import SimplePlayer\n",
    "from simulation_utils import *\n",
    "import os\n",
    "\n",
    "\n",
    "def train_simulation(environment, train_agent, target_agent, n_games, update_step, test_agents=None, test_games=None):\n",
    "    \n",
    "    # Create a test environment\n",
    "    if test_agents is not None:\n",
    "        test_env = clone(environment)\n",
    "\n",
    "    # Set the players to the environment\n",
    "    environment.player1 = train_agent\n",
    "    environment.player2 = target_agent\n",
    "\n",
    "    # Get the directory to save/load models and log information\n",
    "    model_dir = model_path(environment.size)\n",
    "    log_file = log_path(environment.size)\n",
    "\n",
    "    # For Debugging\n",
    "    print (\"Model Directory is: {}\".format(model_dir))\n",
    "    print (\"Log file is: {}\".format(log_file))\n",
    "\n",
    "    # Start log file if it doesn't exist, otherwise load from last game\n",
    "    if not os.path.exists(log_file) or os.stat(log_file) == 0:\n",
    "        with open(log_file,'a') as file:\n",
    "            game_start = 1\n",
    "            file.write('Game Number,Test Agent,Win Percentage,Draw Percentage,Loss Percentage\\n')\n",
    "    else:\n",
    "        last_line = recent_game(log_file)\n",
    "        if last_line != \"Game Number\":\n",
    "            game_start = int(recent_game(log_file)) + 1\n",
    "        else:\n",
    "            game_start = 1\n",
    "\n",
    "    train_agent.initialize_network()\n",
    "    target_agent.initialize_network()\n",
    "\n",
    "    # Load previous model if it exists\n",
    "    try:\n",
    "        train_agent.load_model(model_dir + '-' + str(game_start - 1))\n",
    "        target_agent.load_model(model_dir + '-' + str(game_start - 1))\n",
    "        print(\"Load Succeeded\")\n",
    "    except:\n",
    "        print(\"Attempted load and failed\")\n",
    "\n",
    "    # Debugging\n",
    "    print (\"Starting at game {}\".format(game_start))\n",
    "\n",
    "    # Begin training games\n",
    "    for game_number in range(game_start, n_games + 1):\n",
    "\n",
    "        # Switch who goes first every other round\n",
    "        environment.player1 = train_agent\n",
    "        environment.player2 = target_agent\n",
    "        if game_number % 2 == 0:\n",
    "            switch_players(environment)\n",
    "\n",
    "        environment.play()\n",
    "\n",
    "        # Write to logs every 1 games\n",
    "        if game_number % 200 == 0 and test_agents:\n",
    "            print(\"Game {} Test Results\".format(game_number))\n",
    "            with open(log_file, 'a') as file:\n",
    "                for agent in test_agents:\n",
    "                    win_percentage, draw_percentage, loss_percentage = test(test_env, train_agent, agent, test_games)\n",
    "                    file.write('{},{},{},{},{}\\n'.format(game_number, agent, win_percentage, draw_percentage, loss_percentage))\n",
    "                    print()\n",
    "\n",
    "        \n",
    "        # Play games agains the old model\n",
    "        if game_number % update_step == 0:\n",
    "\n",
    "            # Give the target agent the most recent model\n",
    "            print(\"Saving current model\")\n",
    "            path = train_agent.save_model(model_dir, global_step=game_number)\n",
    "\n",
    "            # Load model into target\n",
    "            print (\"Loading model into target\")\n",
    "            target_agent.load_model(path)\n",
    "            print ()\n",
    "\n",
    "    print (\"Finished!\")\n",
    "\n",
    "\n",
    "def output_comparison(training_games=10000, test_games=1000):\n",
    "\n",
    "    training_env = DotsAndBoxes(3)\n",
    "    training_env2 = clone(training_env)\n",
    "    test_env = clone(training_env)\n",
    "\n",
    "    tanh_player = DQNLearner('tanh output', alpha=1e-6, gamma=0.6)\n",
    "    linout_player = DQNLearner('linear output', alpha=1e-6, gamma=0.6)\n",
    "    training_opponent = Player('training opponent')\n",
    "    training_opponent2 = Player('training opponent 2')\n",
    "\n",
    "    training_env.player1 = tanh_player\n",
    "    training_env.player2 = training_opponent\n",
    "    tanh_player.initialize_network(output='tanh')\n",
    "\n",
    "    training_env2.player1 = linout_player\n",
    "    training_env2.player2 = training_opponent2\n",
    "    linout_player.initialize_network(output='linear')\n",
    "\n",
    "    test_random = Player('Random')\n",
    "    test_moderate = SimplePlayer('Moderate', level=1)\n",
    "    test_advanced = SimplePlayer('Advanced', level=2)\n",
    "\n",
    "    log_file = '.{0:s}Analysis{0:s}output_comparison.txt'.format(os.sep)\n",
    "    with open(log_file, 'w') as file:\n",
    "        file.write('Learning Agent,Test Agent,Win %, Draw %, Loss %\\n')\n",
    "\n",
    "    for game_number in range(1, training_games+1):\n",
    "\n",
    "        # Switch who goes first every other round\n",
    "        training_env.player1 = tanh_player\n",
    "        training_env.player2 = training_opponent\n",
    "\n",
    "        training_env2.player1 = linout_player\n",
    "        training_env2.player2 = training_opponent2\n",
    "\n",
    "        # Switch starting positions\n",
    "        if game_number % 2 == 0:\n",
    "            switch_players(training_env)\n",
    "            switch_players(training_env2)\n",
    "\n",
    "        training_env.play()\n",
    "        training_env2.play()\n",
    "\n",
    "        if game_number % (training_games/20) == 0:\n",
    "            print(\"Running Tests at game {}\".format(game_number))\n",
    "            for test_agent in (test_random, test_moderate, test_advanced):\n",
    "                for player in (tanh_player, linout_player):\n",
    "                    print(\"Testing player: {}\".format(player))\n",
    "                    wins, draws, loss = test(test_env, player, test_agent, test_games)\n",
    "                    with open(log_file, 'a') as file:\n",
    "                        file.write('{},{},{},{},{}\\n'.format(player, test_agent, wins, draws, loss))\n",
    "                    print()\n",
    "\n",
    "    print (\"Training Completed!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    game_size = 3\n",
    "    train_agent = DQNLearner('train',alpha=1e-6,gamma=0.6)\n",
    "\n",
    "    target_agent = DQNLearner('target')\n",
    "    target_agent.learning = False\n",
    "\n",
    "    # Load the testing agents\n",
    "    test_agent1 = Player(name='random_player')\n",
    "    test_agent2 = SimplePlayer(name='moderate_player', level=1)\n",
    "    test_agent3 = SimplePlayer(name='advanced_player', level=2)\n",
    "    \n",
    "    env = DotsAndBoxes(game_size)\n",
    "    n_games = 10000\n",
    "    update_step = 200\n",
    "    test_games = 1000\n",
    "    \n",
    "    train_simulation(env, train_agent, target_agent,\n",
    "                                    n_games, update_step,\n",
    "                                    [test_agent1,test_agent2,test_agent3], test_games)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
